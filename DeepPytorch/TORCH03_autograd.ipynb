{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH 03. Autograd: automotic differentiation\n",
    "- `autograd` package provides automatic differentiation for all operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor and set `requires_grad=True` to track computation with it\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "<AddBackward0 object at 0x00000181244EBF98>\n",
      "\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do a tensor operation\n",
    "y = x + 2\n",
    "print(y, end='\\n\\n')\n",
    "\n",
    "# y was created as a result of an operation, so it has a `grad_fn`\n",
    "print(y.grad_fn, end='\\n\\n')\n",
    "\n",
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x00000181244F6160>\n"
     ]
    }
   ],
   "source": [
    "# `.requires_grad_( ... )` changes an existing Tensor's `requires_grad` flag in-place.\n",
    "# The input flag defaults to `False` if not given.\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)    # default is False\n",
    "a.requires_grad_(True)    # Set requres_grad as True\n",
    "print(a.requires_grad)    # It will be a True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)          # Since requires_grad is True, exists grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since `out` contains a single scalar,\n",
    "# `out.backward()` is equivalent to `out.backward(torch.tensor(1,))`.\n",
    "out.backward()  # 역전파 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Print gradients d(out)/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$o = {\\cfrac{1}{4}}{\\sum_{i}{z_{i}}}$$\n",
    "$$z_{i} = 3(x_{i}+2)^{2}$$\n",
    "$$z_{i}|_{x_{i}=1}=27$$\n",
    "$$\\text{Therefore,}{\\;}\\cfrac{{\\partial}o}{{\\partial}x_{i}}=\\cfrac{3}{2}(x_{i}+2)$$\n",
    "$${\\cfrac{{\\partial}o}{{\\partial}x_{i}}}\\bigg{|}_{x_{i}=1}=\\cfrac{9}{2}=4.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Mathematically, if you have a vector valued function}\\;\\vec{y}=f(\\vec{x}),$$\n",
    "$$\\text{then the gradient of}\\;\\vec{y}\\text{ with respect to}\\;\\vec{x}\\text{ is a jacobian matrix:}$$\n",
    "$$J=\\begin{pmatrix}\n",
    "\\cfrac{{\\partial}y_{1}}{{\\partial}x_{1}} & \\cdots & \\cfrac{{\\partial}y_{1}}{{\\partial}x_{n}}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\cfrac{{\\partial}y_{m}}{{\\partial}x_{1}} & \\cdots & \\cfrac{{\\partial}y_{m}}{{\\partial}x_{n}}\\\\\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Generally speaking, `torch.autograd` is an engine for computing vector-Jacobian product.}$$\n",
    "$$\\text{That is, given any vector }v=(v1{\\quad}v2{\\quad}{\\cdots}{\\quad}v_{m})^{T}\\text{, compute the product }v^{T}{\\cdot}J$$\n",
    "$$\\text{If }v\\text{ happens to be the gradient of a scalar function }l=g\\big{(}\\vec{y}\\big{)}\\text{, that is, }v=\\bigg{(}\\cfrac{{\\partial}l}{{\\partial}y_{1}}\\;\\cdots\\;\\cfrac{{\\partial}l}{{\\partial}y_{m}}\\bigg{)}^{T}\\text{,}$$\n",
    "$$\\text{then by the chain rule, the vector-Jacobian product would be the gradient of }l\\text{ with respect to }\\vec{x}\\text{:}$$\n",
    "$$J^{T}\\cdot{v}=\\begin{pmatrix}\n",
    "\\cfrac{{\\partial}y_{1}}{{\\partial}x_{1}} & \\cdots & \\cfrac{{\\partial}y_{m}}{{\\partial}x_{1}}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\cfrac{{\\partial}y_{1}}{{\\partial}x_{n}} & \\cdots & \\cfrac{{\\partial}y_{m}}{{\\partial}x_{n}}\\\\\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\cfrac{{\\partial}l}{{\\partial}y_{1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\cfrac{{\\partial}l}{{\\partial}y_{m}}\\\\\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "\\cfrac{{\\partial}l}{{\\partial}x_{1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\cfrac{{\\partial}l}{{\\partial}x_{n}}\\\\\n",
    "\\end{pmatrix}$$\n",
    "$$\\text{(Note that }v^{T}\\cdot{J}\\text{ gives a row vector which can be treated as a column vector by taking }{J}^{T}\\cdot{v}\\text{)}$$\n",
    "$$\\text{This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : tensor([-0.4659,  2.4516,  0.8795], requires_grad=True)\n",
      "y : tensor([-238.5540, 1255.2350,  450.3279], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# vector-Jacobian product example\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print('x :', x)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print('y :', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "# y is not a scalar,\n",
    "# `torch.autograd` could not compute the full jacobian directly,\n",
    "# but if we just want the vector-jacobian product,\n",
    "# simply pass the vector to `backward` as argument.\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Stop autograd from tracking history on Tensors with `.requires_grad=True`\n",
    "# By wrapping the code block in `with torch.no_grad():`\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Or by using `.detach()` to get a new Tensor with the same content\n",
    "# but that does not require gradients\n",
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE CODE FOR TORCH.AUTOGRAD\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.autograd.function import Function, NestedIOFunction\n",
    "from torch.autograd.gradcheck import gradcheck, gradgradcheck\n",
    "from torch.autograd.grad_mode import no_grad, enable_grad, set_grad_enabled\n",
    "from torch.autograd.anomaly_mode import detect_anomaly, set_detect_anomaly\n",
    "from torch.autograd import profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['Variable', 'Function', 'backward', 'grad_modea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_grads(outputs, grads):\n",
    "    new_grads = []\n",
    "    for out, grad in zip(outputs, grads):\n",
    "        # Gradient가 torch.Tensor객체 일 경우\n",
    "        if isinstance(grad, torch.Tensor):\n",
    "            # out과 grad의 shape 체크\n",
    "            if not out.shape == grad.shape:\n",
    "                raise RuntimeError(\"Mismatch in shape: grad_output[\"\n",
    "                                   + str(grads.index(grad)) + \"] has a shape of \"\n",
    "                                   + str(grad.shape) + \" and output[\"\n",
    "                                   + str(outputs.index(out)) + \"] has a shape of \"\n",
    "                                   + str(out.shape) + \".\")\n",
    "            new_grads.append(grad)\n",
    "        # Gradient가 None일 경우\n",
    "        elif grad is None:\n",
    "            # requires_grad == True :\n",
    "            if out.requires_grad:\n",
    "                # out이 scalar가 아닐 경우 에러 처리\n",
    "                if out.numel() != 1:\n",
    "                    '''\n",
    "                    # Returns the total number of elements in the `input` tensor.\n",
    "                    >>> a = torch.randn(1, 2, 3, 4, 5)\n",
    "                    >>> torch.numel(a)\n",
    "                    120\n",
    "                    >>> a = torch.zeros(4, 4)\n",
    "                    >>> torch.numel(a)\n",
    "                    16\n",
    "                    '''\n",
    "                    raise RuntimeError(\"grad can be implicitly created only for scalar outputs\")\n",
    "                \n",
    "                new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))\n",
    "            # requires_grad == False : None 추가\n",
    "            else:\n",
    "                new_grads.append(None)\n",
    "        # Gradient가 torch.Tensor 혹은 None이 아닐 경우 에러처리\n",
    "        else:\n",
    "            raise TypeError(\"gradients can be either Tensors or None, but got \" +\n",
    "                            type(grad).__name__)\n",
    "    # tuple로 return\n",
    "    return tuple(new_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False,\n",
    "             grad_variables=None):\n",
    "    \"\"\"\n",
    "    Computes the sum of gradients of given tensors w.r.t. graph leaves.\n",
    "    \"\"\"\n",
    "    if grad_variables is not None:\n",
    "        warnings.warn(\"'grad_variables' is deprecated. Use 'grad_tensors' instead.\")\n",
    "        if grad_tensors is None:\n",
    "            grad_tensors = grad_variables\n",
    "        else:\n",
    "            raise RuntimeErorr(\"'grad_tensors' and 'grad_variables' (deprecated) \"\n",
    "                               \"arguments both passed to backward(). Please only \"\n",
    "                               \"use 'grad_tensors'.\")\n",
    "    \n",
    "    tensors = (tensors, ) if isinstance(tensors, torch.Tensor) else tuple(tensors)\n",
    "    \n",
    "    if grad_tensors is None:\n",
    "        grad_tensors = [None] * len(tensors)\n",
    "    elif isinstance(grad_tensors, torch.Tensor):\n",
    "        grad_tensors = [grad_tensors]\n",
    "    else:\n",
    "        grad_tensors = list(grad_tensors)\n",
    "    \n",
    "    grad_tensors = _make_grads(tensors, grad_tensors)\n",
    "    if retain_graph is None:\n",
    "        retain_graph = create_graph\n",
    "        \n",
    "    # 위에서 설정만 잡아주고 돌리는건 C++ Imperative Engine에서 돌린다.\n",
    "    Variable._execution_engine.run_backward(\n",
    "        tensors, grad_tensors, retain_graph, create_graph,\n",
    "        allow_unreachable=True)  # allow_unreachable flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False,\n",
    "         only_inputs=True, allow_unused=False):\n",
    "    \"\"\"\n",
    "    Computes and returns the sum of gradients of outputs w.r.t. the inputs.\n",
    "    \"\"\"\n",
    "    if not only_inputs:\n",
    "        warnings.warn(\"only_inputs argument is deprecated and is ignored now \"\n",
    "                      \"(defualts to True). To accumulate gradient for other \"\n",
    "                      \"parts of the graph, please use torch.autograd.backward.\")\n",
    "    \n",
    "    outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)\n",
    "    inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)\n",
    "    \n",
    "    if grad_outpus is None:\n",
    "        grad_outputs = [None] * len(outputs)\n",
    "    elif isinstance(grad_outputs, torch.Tensor):\n",
    "        grad_outputs = [grad_outputs]\n",
    "    else:\n",
    "        grad_outputs = list(grad_outputs)\n",
    "    \"\"\"\n",
    "    아니, 지금 elif랑 else랑 차이가 뭐야?\n",
    "    ```python\n",
    "    >>> a\n",
    "    tensor([[1.8083, 1.6985],\n",
    "            [2.0055, 1.6993]], requires_grad=True)\n",
    "    >>> [a]\n",
    "    [tensor([[1.8083, 1.6985],\n",
    "             [2.0055, 1.6993]], requires_grad=True)]\n",
    "    >>> list(a)\n",
    "    [tensor([1.8083, 1.6985], grad_fn=<SelectBackward>),\n",
    "     tensor([2.0055, 1.6993], grad_fn=<SelectBackward>)]\n",
    "    ```\n",
    "    때문에 위와 같이 다르게 처리해줘야한다!\n",
    "    \"\"\"\n",
    "    grad_outputs = _make_grads(outputs, grad_outputs)\n",
    "    \n",
    "    if retain_graph is None:\n",
    "        retain_graph = create_graph\n",
    "    \n",
    "    return Variable._execution_engine.run_backward(\n",
    "        outputs, grad_outputs, retain_graph, create_graph,\n",
    "        inputs, allow_unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies in case of gradient checkpointing for memory\n",
    "# optimization. Currently, for gradient checkpointing, we only support imperative\n",
    "# backwards call i.e. torch.autograd.backward() and the torch.autograd.grad() won't\n",
    "# work. The reason being that: torch.autograd.grad() only calculates the grads\n",
    "# for the inputs that are passed by user but it doesn't calculate grad for\n",
    "# anything else e.g. model parameters like weights, bias etc. However, for\n",
    "# torch.autograd.backward(), we would actually compute the grad for the weights as well.\n",
    "#\n",
    "# This function returns whether the checkpointing is valid i.e. torch.autograd.backward\n",
    "# or not i.e. torch.autograd.grad. The implementation works by maintaining a thread\n",
    "# local variable in torch/csrc/autograd/engine.cpp which looks at the NodeTask\n",
    "# in the stack and before a NodeTask is executed in evaluate_function, it\n",
    "# checks for whether reentrant backwards is imperative or not.\n",
    "# See https://github.com/pytorch/pytorch/pull/4594 for more discussion/context\n",
    "def _is_checkpoint_valid():\n",
    "    return Variable._execution_engine.is_checkpoint_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable(*args, **kwargs):\n",
    "    warnings.warn(\"torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\")\n",
    "    return torch.tensor(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if not torch._C._autograd_init():\n",
    "    raise RuntimeError(\"autograd initialization failed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally disabling gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.autograd.no_grad`\n",
    "- Context-manager that disabled gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.no_grad"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-aa1f7f2b9743>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/pytorch/issues/17345\n",
    "Bug in here:\n",
    "```\n",
    "pytorch/torch/multiprocessing/reductions.py\n",
    "\n",
    "83        t = torch.nn.parameter.Parameter(t) \n",
    "84    t.requires_grad = requires_grad \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x * 2\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def doubler(x):\n",
    "    return x * 2\n",
    "\n",
    "z = doubler(x)\n",
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.autograd.enable_grad`\n",
    "- Context-manager that enables gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.enable_grad"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enable_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.enable_grad():\n",
    "        y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.enable_grad()\n",
    "def doubler(x):\n",
    "    return x * 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = doubler(x)\n",
    "    \n",
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.autograd.set_grad_enabled`\n",
    "- Context-manager that sets gradient calculation to on or off\n",
    "- When using `enabled_grad` context manager, `set_grad_enabled(False)` has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "is_train = False\n",
    "\n",
    "with torch.set_grad_enabled(is_train):\n",
    "    y = x * 2\n",
    "    \n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응용해보자\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "@torch.enable_grad()\n",
    "def linear_transform(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "def linear_transform2(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "is_train = False\n",
    "with torch.set_grad_enabled(is_train):\n",
    "    y = linear_transform(x, 2, 3)\n",
    "    z = linear_transform2(x, 2, 3)\n",
    "\n",
    "y.requires_grad, z.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations on Tensors\n",
    "- `Variable` is deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Tensor(torch._C._TensorBase):\n",
    "    ...\n",
    "    def backward(self, gradient=None, retain_graph=None, create_graph=False):\n",
    "        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
    "    ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
